---

# Step 0a : set kubeconfig_path, OCP version vars and dci_topic. Check hooks
- name: "Execute kubeconfig step"
  hosts: jumphost
  tags:
    - kubeconfig
  environment:
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: Kubeconfig tasks
      block:
        - name: Display proxy settings
          ansible.builtin.debug:
            msg: "http_proxy={{ lookup('env', 'http_proxy') }} https_proxy={{ lookup('env', 'https_proxy') }} no_proxy={{ lookup('env', 'no_proxy') }}"

        - name: Check kubeconfig
          ansible.builtin.include_tasks: 'plays/check_kubeconfig.yml'

        # Check prerequisite
        - name: "Check prerequisite"
          ansible.builtin.include_tasks: 'plays/check_prerequisite.yml'

      rescue: &error_with_upload_logs
        - name: "Run common logging process"
          ansible.builtin.include_tasks:
            file: plays/common_logging.yml
            apply:
              environment:
                KUBECONFIG: "{{ kubeconfig_path }}"

        - name: "Execute the teardown process"
          ansible.builtin.include_tasks:
            file: "{{ dci_config_dir }}/hooks/teardown.yml"
            apply:
              environment:
                KUBECONFIG: "{{ kubeconfig_path }}"
          when:
            - dci_teardown_on_failure
            - check_teardown.stat.exists

        - name: "Run the teardown play"
          ansible.builtin.include_tasks:
            file: plays/teardown.yml
            apply:
              environment:
                KUBECONFIG: "{{ kubeconfig_path }}"
          when:
            - dci_teardown_on_failure

        - name: "Execute the error process"
          ansible.builtin.include_tasks: plays/error.yml

        - name: Run agent cleanup
          include_tasks: plays/agent-cleanup.yml

        # The error may happen before creating the job_info variable, or it may happen in a
        # job where dci tag is not enabled (so job_info variable is not created). Then,
        # we have to set a different error message in that case
        - name: Fail properly
          ansible.builtin.fail:
            msg: "Error: Something went wrong"
          when: job_info is not defined

        # Use dci tag to avoid problems with job_info.job.id, which would not be defined
        # if dci tag is not enabled
        - name: Fail properly
          ansible.builtin.fail:
            msg: "Error: Something went wrong, review the log at: https://www.distributed-ci.io/jobs/{{ job_info.job.id }}/jobStates"
          tags: [dci]

# Step 0b : initial step
- name: "Execute initial step"
  hosts: localhost
  gather_facts: false
  tags:
    - job
    - dci
  environment:
    TMPDIR: "{{ hostvars['jumphost']['ansible_env']['HOME'] }}/tmp"
  tasks:
    - name: Read credentials from env vars
      ansible.builtin.set_fact:
        dci_client_id: "{{ lookup('env', 'DCI_CLIENT_ID') }}"
        dci_api_secret: "{{ lookup('env', 'DCI_API_SECRET') }}"
        dci_cs_url: "{{ lookup('env', 'DCI_CS_URL') }}"
        dci_ui_url: "{{ lookup('env', 'DCI_UI_URL') | default('https://www.distributed-ci.io', True) }}"
      no_log: true

    - name: Check job topic
      ansible.builtin.assert:
        that:
          - "dci_topic == job_info.job.topic.name"
        fail_msg: "dci_topic is inconsistent: {{ dci_topic }} != {{ job_info.job.topic.name }}. Use dci-pipeline-schedule to schedule jobs."
        success_msg: "dci_topic is consistent"
      when:
        - dci_topic is defined
        - job_info is defined

    # Add the ocp component when called from dci-pipeline
    - name: Attach ocp component to the job
      dci_job_component:
        job_id: "{{ job_info.job.id }}"
        component_id: "{{ ocp_component_id }}"
      register: job_component_result
      until: job_component_result is not failed
      retries: 5
      delay: 20
      when:
        - job_info is defined
        - ocp_component_id is defined

    # Schedule a new job only if not passed via dci-pipeline
    - name: "Schedule a new job"
      dci_job:
        components: "{{ dci_components + [ocp_component_id] | default([ocp_component_id]) }}"
        components_by_query: "{{ dci_components_by_query | default([]) }}"
        topic: "{{ dci_topic }}"
        comment: "{{ dci_comment }}"
        url: "{{ dci_url }}"
        name: "{{ dci_name }}"
        configuration: "{{ dci_configuration }}"
        previous_job_id: "{{ dci_previous_job_id }}"
        team_id: "{{ dci_team_id }}"
        pipeline_id: "{{ dci_pipeline_id }}"
      register: job_info
      when: job_info is not defined

    - name: Set job id
      ansible.builtin.set_fact:
        job_id: "{{ job_info.job.id }}"

    - name: Copy the job_id to the JOB_ID_FILE if it exists
      ansible.builtin.copy:
        content: "{{ job_id }}"
        dest: "{{ JOB_ID_FILE }}"
        mode: '0644'
      when: JOB_ID_FILE is defined

    - name: Set previous job id
      ansible.builtin.set_fact:
        dci_previous_job_id: "{{ job_info.job.previous_job_id }}"
      when:
        - 'dci_previous_job_id is not defined'
        - 'job_info.job.previous_job_id != None'
        - 'job_info.job.previous_job_id | length > 0'

    - name: 'Set DCI tags for the current job'
      dci_job:
        id: '{{ job_id }}'
        tags: >
          {{
            [ (dci_disconnected | default(false) | bool) | ternary("disconnected", "connected") ] +
            ["agent:openshift-app"] +
            dci_tags +
            dci_workarounds
          }}

    - name: Count workarounds
      dci_job:
        id: "{{ job_id }}"
        key: "workarounds"
        value: "{{ dci_workarounds | length }}"

    - name: Add cluster tag to the current job
      dci_job:
        id: '{{ job_id }}'
        tags:
          - cluster:{{ cluster_name }}
      when: cluster_name is defined

    - name: "Merge pull secrets"
      ansible.builtin.include_tasks: 'plays/pullsecrets.yml'
      when: dci_disconnected | default(false) | bool

    # Keep in sync with test-runner parsing from d-o-a
    - name: UI URL
      ansible.builtin.debug:
        msg: 'Follow the log at: {{ dci_ui_url }}/jobs/{{ job_id }}/jobStates' # noqa 204

    - name: Set facts also in jumphost host scope
      ansible.builtin.set_fact:
        job_info: "{{ job_info }}"
        job_id: "{{ job_id }}"
        dci_client_id: "{{ dci_client_id }}"
        dci_api_secret: "{{ dci_api_secret }}"
        dci_cs_url: "{{ dci_cs_url }}"
        dci_ui_url: "{{ dci_ui_url }}"
        pullsecret_tmp_file: "{{ pullsecret_tmp_file | default() }}"
        ocp_component_id: "{{ ocp_component_id }}"
      delegate_to: jumphost
      delegate_facts: true

# Step 1a : Red Hat "pre-run" step
- name: "Execute Red Hat pre-run step"
  hosts: jumphost
  tags:
    - pre-run
    - redhat-pre-run
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: Pre-run tasks
      block:

        - name: "Setup job_logs directory"
          ansible.builtin.include_tasks: plays/log-dir.yml
          when: job_logs is undefined

        - name: "Execute pre-run"
          ansible.builtin.include_tasks: 'plays/pre-run.yml'

      rescue: *error_with_upload_logs

# Step 1b : Hook "pre-run" step
- name: "Execute hooks' pre-run step"
  hosts: jumphost
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tags:
    - pre-run
    - partner-pre-run
  tasks:
    - name: Pre-run hooks
      block:

        - name: "Setup job_logs directory"
          ansible.builtin.include_tasks: plays/log-dir.yml
          when: job_logs is undefined

        - name: Run the pre-run hook
          ansible.builtin.include_tasks: '{{ dci_config_dir }}/hooks/pre-run.yml'
          when: check_pre_run.stat.exists

      rescue: *error_with_upload_logs

# Step 2 : "running" step
- name: "Execute install step"
  hosts: jumphost
  tags:
    - install
    - running
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: Install tasks
      block:
        - name: Set job state - running
          dci_job:
            id: "{{ job_id }}"
            status: "running"
          delegate_to: localhost
          tags: [dci]

        - name: "Execute install play"
          ansible.builtin.include_tasks: 'plays/install.yml'

        - name: "Execute install hook"
          ansible.builtin.include_tasks: "{{ dci_config_dir }}/hooks/install.yml"

        - name: "Execute post-install play"
          ansible.builtin.include_tasks: 'plays/post-install.yml'

      rescue: &failure_with_upload_logs
        - name: "Run common logging process"
          ansible.builtin.include_tasks: plays/common_logging.yml

        - name: "Execute the teardown process"
          ansible.builtin.include_tasks: "{{ dci_config_dir }}/hooks/teardown.yml"
          when:
            - dci_teardown_on_failure
            - check_teardown.stat.exists

        - name: "Run the teardown play"
          ansible.builtin.include_tasks: plays/teardown.yml
          when:
            - dci_teardown_on_failure

        - name: "Execute the failure process"
          ansible.builtin.include_tasks: plays/failure.yml

        - name: Run agent cleanup
          include_tasks: plays/agent-cleanup.yml

        # The error may happen before creating the job_info variable, or it may happen in a
        # job where dci tag is not enabled (so job_info variable is not created). Then,
        # we have to set a different error message in that case
        - name: Fail properly
          ansible.builtin.fail:
            msg: "Failure: Something went wrong"
          when: job_info is not defined

        # Use dci tag to avoid problems with job_info.job.id, which would not be defined
        # if dci tag is not enabled
        - name: Fail properly
          ansible.builtin.fail:
            msg: "Failure: Something went wrong, review the log at: https://www.distributed-ci.io/jobs/{{ job_info.job.id }}/jobStates"
          tags: [dci]

# Step 3a : "testing" step
- name: "Execute Red Hat tests step"
  hosts: jumphost
  tags:
    - running
    - testing
    - redhat-testing
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: Tests tasks
      block:

        - name: "Setup job_logs directory"
          ansible.builtin.include_tasks: plays/log-dir.yml
          when: job_logs is undefined

        - name: "Execute Red Hat tests"
          ansible.builtin.include_tasks: plays/tests.yml

      rescue: *failure_with_upload_logs

# Step 3b : "testing" step
- name: "Execute partner tests step"
  hosts: jumphost
  tags:
    - running
    - testing
    - partner-testing
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: Test hooks
      block:

        - name: "Setup job_logs directory"
          ansible.builtin.include_tasks: plays/log-dir.yml
          when: job_logs is undefined

        - name: "Execute tests hook"
          ansible.builtin.include_tasks: "{{ dci_config_dir }}/hooks/tests.yml"
          when: check_tests.stat.exists

      rescue: *failure_with_upload_logs

# Step 3c : "testing" step
- name: "Check test results step"
  hosts: jumphost
  tags:
    - running
    - testing
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: Test results
      block:

        - name: "Check test results"
          ansible.builtin.include_tasks: plays/check-test-results.yml

      rescue: *failure_with_upload_logs

# Step 4 : "post-run" step
- name: "Execute post-run step"
  hosts: jumphost
  tags:
    - post-run
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: Post-run tasks
      block:
        - name: Run the post-run hook
          ansible.builtin.include_tasks: '{{ dci_config_dir }}/hooks/post-run.yml'
          when: check_post_run.stat.exists

        - name: "Run post-run"
          ansible.builtin.include_tasks: plays/post-run.yml

      rescue: *error_with_upload_logs

# Step 5: "Final step (success)"
- name: "Success"
  hosts: jumphost
  tags:
    - success
  environment:
    KUBECONFIG: "{{ kubeconfig_path }}"
    TMPDIR: "{{ ansible_env.HOME }}/tmp"
  tasks:
    - name: "Run common logging process"
      ansible.builtin.include_tasks: plays/common_logging.yml

    - name: "Execute the teardown process"
      ansible.builtin.include_tasks: "{{ dci_config_dir }}/hooks/teardown.yml"
      when:
        - dci_teardown_on_success
        - check_teardown.stat.exists

    - name: "Run the teardown play"
      ansible.builtin.include_tasks: plays/teardown.yml
      when:
        - dci_teardown_on_success

    - name: Run agent cleanup
      include_tasks: plays/agent-cleanup.yml

    - name: Success
      dci_job:
        id: "{{ job_id }}"
        status: "success"
      delegate_to: localhost
      tags: [dci]

    - name: "Final step"
      ansible.builtin.debug:
        msg: "The job is now finished. Review the log at: https://www.distributed-ci.io/jobs/{{ job_info.job.id }}/jobStates"
      tags: [dci]
...
